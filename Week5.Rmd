---
title: "Week 5"
author: "Yuqi Gao"
date: "2022-11-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#install.packages("MASS")
#install.packages("rgl")
library(tidyverse)
library(MASS)
library(rgl) 
```
```{r}
data("hills") # Loads the hills dataset into a variable "hills"
nrow(hills)
```

```{r}
hills_train <- hills[1:30,]
hills_test <- hills[31:35,]
```

```{r}
hills_train[1:10,]
```

```{r}
summary(hills_train)
```

```{r}
ggplot(data=hills_train,aes(x=dist, y=time)) + geom_point()
```

```{r}
ggplot(data=hills_train, aes(x=climb, y=time)) + geom_point()
```

#### For dist
```{r}
cor.test(hills_train$dist, hills_train$time)
```
#### For Climb
```{r}
cor.test(hills_train$climb, hills_train$time)
```

```{r}
mod_dist <- lm(formula=time~dist, data=hills_train)# predicting time using dist
```

```{r}
summary(mod_dist)
```

```{r}
coef(mod_dist)
```

```{r}
mod_climb <- lm(formula=time~climb, data=hills_train)
summary(mod_climb)
coef(mod_climb)
```


```{r}
coefs_dist <- coef(mod_dist)
ggplot(data=hills_train, aes(x=dist, y=time)) + geom_point() + geom_abline(mapping=aes(slope=coefs_dist["dist"], intercept=coefs_dist["(Intercept)"]), color='cyan')
```

```{r}
hills_resid <- hills_train # make a copy of the dataset, to leave the original untouched
hills_resid$predicted <- predict(mod_dist) # if data are not specified, uses the data the model was fit to
hills_resid$residuals <- residuals(mod_dist)# show the data with predicted and residual values
hills_resid[1:10,]
```

```{r}
ggplot(data=hills_resid, aes(x=dist,y=time)) +
 geom_point(size=2) + # make the actual values show up more clearly
 geom_point(size=2, aes(y=predicted), shape=3) + # show the predicted values
 geom_segment(aes(xend=dist, yend=predicted), alpha=0.9, color='purple') +
 geom_abline(mapping=aes(
 slope=coefs_dist["dist"],
 intercept=coefs_dist["(Intercept)"]
 ), color='cyan')
```

```{r}
plot(mod_dist, which = 1)
```

```{r}
plot(mod_climb, which = 1)
```

```{r}
predict(mod_dist, newdata=hills_test)
```

```{r}
predict(mod_dist,newdata=hills_test, interval='confidence')
```

```{r}
hills_dist_test <- hills_test # make a copy of the test data to leave the original unaltered
hills_dist_test$predicted <- predict(mod_dist, newdata=hills_dist_test)
hills_dist_test$residuals <- hills_dist_test$predicted - hills_dist_test$time
hills_dist_test
```

```{r}
ggplot(data=hills_dist_test, aes(x=dist,y=time)) +
geom_point(size=3) + # make the actual values show up more clearly
 geom_point(size=2, aes(y=predicted), shape=1) + # show the predicted values
 geom_segment(aes(xend=dist, yend=predicted), alpha=0.9, color='red') +
 geom_abline(mapping=aes(
 slope=coefs_dist["dist"],
 intercept=coefs_dist["(Intercept)"]
 ), color='purple')
```

```{r}
sse_dist <- sum(hills_dist_test$residuals**2)
sse_dist
```

```{r}
predict(mod_climb,newdata=hills_test, interval='confidence')
hills_climb_test <- hills_test # make a copy of the test data to leave the original unaltered
hills_climb_test$predicted <- predict(mod_climb, newdata=hills_climb_test)
hills_climb_test$residuals <- hills_climb_test$predicted - hills_climb_test$time
sse_climb <- sum(hills_climb_test$residuals**2)
sse_climb
```

```{r}
ggplot(data=hills_train, aes(x=dist, y=climb)) + geom_point()
```

```{r}
cor.test(hills_train$dist, hills_train$climb)
```

```{r}
library(rgl)
plot3d(
 x=hills_train$dist,
 y=hills_train$climb,
 z=hills_train$time
)
rglwidget() # this should open an interactive 3-d widget in the Viewer pane
```

```{r}
mod_hills <- lm(formula=time~climb+dist, data=hills_train)
```

```{r}
 plot3d(
 x=hills_train$dist,
 y=hills_train$climb,
 z=hills_train$time,
 type='s', size=2, col='red' # show the data points as big blue spheres for visibility
 )
coefs <- coef(mod_hills)
planes3d(a=coefs["dist"],b=coefs["climb"],c=-1,d=coefs["(Intercept)"], col='cyan', alpha=0.5)
# this uses the model coefficients to plot the regression plane in 3-d space
rglwidget()
```

```{r}
data("iris") 
```

```{r}
iris <- iris[sample(1:nrow(iris)),]
train_size = 0.7 # using 70% of data for training
iris_train <- iris[1:(train_size*nrow(iris)),]
iris_test <- iris[(nrow(iris_train)+1):nrow(iris),]
head(iris_train)
```

```{r}
colors <- c('#1b9e77', '#d95f02', '#7570b3') # colors chosen to be visually unambiguous: see https://colorbrewer2.org/
iris_train_colors <- colors[as.numeric(iris_train$Species)]
shapes <- c('o', '+', 'x')
iris_train_shapes <- shapes[as.numeric(iris_train$Species)]
ggplot(
 data=iris_train,
 aes(x=Sepal.Length, y=Sepal.Width)
) + geom_point(color=iris_train_colors, shape=iris_train_shapes, size=5)
```

```{r}
binaryColors <- function(data, species) {
 tf_values <- data$Species == species
 color_indices <- as.numeric(tf_values)+1
 return(colors[color_indices])
}
binaryShapes <- function(data, species) {
 tf_values <- data$Species == species
  shape_indices <- as.numeric(tf_values)+1
 return(shapes[shape_indices])
}
```

```{r}
binarySpecies = 'virginica'
ggplot(
 data=iris_train,
 aes(x=Sepal.Length, y=Sepal.Width)
) + geom_point(
 color=binaryColors(iris_train, binarySpecies),
 shape=binaryShapes(iris_train, binarySpecies),
 size=5
)
```

```{r}
 binarySpecies = 'virginica'
ggplot(
 data=iris_train,
  aes(x=Petal.Length, y=Petal.Width)
) + geom_point(
 color=binaryColors(iris_train, binarySpecies),
 shape=binaryShapes(iris_train, binarySpecies),
 size=5
)
```

```{r}
iris_train$binarySpecies <- iris_train$Species == 'virginica'
iris_train$binarySpecies <- iris_train$binarySpecies * 1 # convert from TRUE/FALSE to 1/0
iris_train[1:10,] # examine the data to make sure the binary Species label was set correctly
```

```{r, results='hide', warning=FALSE}
iris_binary_model <- glm(
 binarySpecies ~ Petal.Width+Petal.Length, # predicting the binarySpecies label using petal length/width
 family=binomial(link='logit'), # use a logistic regression
 data=iris_train
)
```

```{r}
binomial_probabilities <- predict(
 iris_binary_model,
 newdata=iris_test,
 type='response'
)
print(binomial_probabilities)
```

```{r}
binomial_predictions <- ifelse(
 binomial_probabilities>0.5,
 1,
 0
)
print(binomial_predictions)
```

```{r}
iris_test$binarySpecies <- iris_test$Species == 'virginica'
iris_test$binarySpecies <- iris_test$binarySpecies * 1 # convert from TRUE/FALSE to 1/0
```

```{r}
binomial_classification_error <- mean(
 binomial_predictions != iris_test$binarySpecies
)
print(paste('Accuracy',1-binomial_classification_error))
```

